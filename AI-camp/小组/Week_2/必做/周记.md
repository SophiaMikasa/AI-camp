# 周记 - 2024年3月25日至3月31日

# 2024年3月25日（周一）

## 今日学习总结

- 将多元线性回归的最小二乘法和数据的部分可视化

## 今日心得体会

我发现用在最小二乘法的矩阵在梯度下降法那用不了了
| 当时的错误代码 |

  ``` python
def fit(self, x, y):
	x_b = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)
    .........
    .........
	elif self.method == 'gd':
		theta = np.random.rand(x_b.shape[1], 1)
		for iteration in range(self.iterations):
			gradients = 2 * (x_b.T.dot(x_b.dot(theta) - y))
			theta = theta - self.eta * gradients
		self.theta = theta
  ```

当时警告是说x_b.dot(theta)不是个一维列数组，但是我也自己算了一下，我算出来是一个一维数组呀，然后就放着不管了（等一天再弄可能思维就不一样了）,在梯度下降法搞出来之前我就用散点图的大致画了一下，发现里面的数据还是挺有意思的，比如说黑人比例与房价的关系，我以为黑人越多房价越便宜，其实不是这样的，恰恰相反，黑人多的地方通常来说房价就会越贵(但是这也不能说有直接的因果关系,还要考虑很多因素的)

------

# 2024年3月26日（周二）

## 今日学习总结

- 将线性回归的的模型评估做了
- 将梯度下降法完成了

## 今日心得体会

今天白天的时候，想着把梯度下降算法给over了，但是还是解决不了，所以先把模型评估给弄好，然后给梯度下降算法给弄好了（这次我不能一口气全部写完，而是分布来计算，不然报错也不知道哪里出错了），但是给梯度下降算法进行模型评估的时候，发现大不如意，得到的r2是负数
| 当时的代码 |

  ``` python
def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4, alpha=0.01):
    self.learning_rate = learning_rate 
    self.max_iter = max_iter
    self.tol = tol
    self.alpha = alpha  # L2 正则化参数
    self.history_learning_rate = []
       
def fit(self, X, y):
    self.theta = np.zeros(X.shape[1])
    self.cost_history = []   
    for epoch in range(1, self.max_iter + 1):
        y_pred = np.dot(X, self.theta)
        loss = np.mean((y_pred - y) ** 2) / 2 + self.alpha * np.sum(self.theta**2) / 2
        self.cost_history.append(loss)
        gradient = np.dot(X.T, (y_pred - y)) / len(y) + self.alpha * self.theta
        self.learning_rate = self.learning_rate / epoch
        self.history_learning_rate.append(self.learning_rate)
        self.theta -= self.learning_rate * gradient
        if len(self.cost_history) > 1 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tol:
            break
  ```

也不知道为什么，加了正则表达式之后，弄出了负相关，于是将正则表达式给删了（所以还是没弄懂为什么我要去掉才可以），才有我现在的梯度下降法，但是这费了我好长时间

------

# 2024年3月27日（周三）

## 今日学习总结

- 今天对多元线性回归的的数据分析进行总结
- 对于逻辑回归的实现

## 今日心得体会

直到下午，我把图反映出来的数据用文字描述了，但是我感觉描述的很抽象而且还不一定正确，所以就那样把，还有我觉得可视化的话，除了散点图我就没有什么想法将其可视化了，因为我觉得散点图可以描述两个变量之间的关系而且还可以描述变量的个数，一共有三个变量，还有热力图，可以描述各个变量之间的相关性，但是其他的图我想不到怎么描述(比如条形图只能描述一个变量的数量)，对，所以我觉得我的可视化还是做的一般般，没有说很清楚。把小组的基础作业弄好之后我就想着做做附加作业了，而且还是担心第三周的大组作业的，因为队列之后就是树了，树的实现我感觉对我来说还是很麻烦的，因为上次的队列和栈都弄得我汗流浃背了；晚上想着完成以下逻辑回归（我在西瓜书那了解到，逻辑回归还可以叫对数几率回归）的作业吧，然后由于我上个星期也提前看过逻辑回归的概念了，所以用起来就很快了，在网上参考了逻辑回归的模型，用了梯度下降法（还参考了别人的L2正则化），只能说梯度下降法如果在多元线性回归实现了的话，那么用在逻辑回归那还是很快的，而且在网上还找了以下数据集，但是也没有深入的分析（比如对逻辑回归模型的评测，当然我知道的有这些测评方法，比如**准确率**，**精确率**，**召回率**，**F1 分数**，**ROC 曲线和AUC**，**混淆矩阵**，但是我这次知识调用了sklearn库的这些方法，并没有做到自己敲代码实现，但我感觉应该能做得出来毕竟已经有公式摆在那了，还有个ROC和AUC图我就不知道怎么画了（以后再研究吧……））,所以今晚就差不多了

------

# 2024年3月28日（周四）

## 今日学习总结

- 

## 今日心得体会



------

# 2024年3月29日（周五）

## 今日学习总结

- 

## 今日心得体会





------

# 2024年3月30日（周六）

## 今日学习总结

- 

## 今日心得体会



------

# 2024年3月31日（周日）

## 今日学习总结

- 

## 今日心得体会

