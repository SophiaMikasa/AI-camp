# 周记 - 2024年3月25日至3月31日

# 2024年3月25日（周一）

## 今日学习总结

- 将多元线性回归的最小二乘法和数据的部分可视化

## 今日心得体会

我发现用在最小二乘法的矩阵在梯度下降法那用不了了
| 当时的错误代码 |

  ``` python
def fit(self, x, y):
	x_b = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)
    .........
    .........
	elif self.method == 'gd':
		theta = np.random.rand(x_b.shape[1], 1)
		for iteration in range(self.iterations):
			gradients = 2 * (x_b.T.dot(x_b.dot(theta) - y))
			theta = theta - self.eta * gradients
		self.theta = theta
  ```

当时警告是说x_b.dot(theta)不是个一维列数组，但是我也自己算了一下，我算出来是一个一维数组呀，然后就放着不管了（等一天再弄可能思维就不一样了）,在梯度下降法搞出来之前我就用散点图的大致画了一下，发现里面的数据还是挺有意思的，比如说黑人比例与房价的关系，我以为黑人越多房价越便宜，其实不是这样的，恰恰相反，黑人多的地方通常来说房价就会越贵(但是这也不能说有直接的因果关系,还要考虑很多因素的)

------

# 2024年3月26日（周二）

## 今日学习总结

- 将线性回归的的模型评估做了
- 将梯度下降法完成了

## 今日心得体会

今天白天的时候，想着把梯度下降算法给over了，但是还是解决不了，所以先把模型评估给弄好，然后给梯度下降算法给弄好了（这次我不能一口气全部写完，而是分布来计算，不然报错也不知道哪里出错了），但是给梯度下降算法进行模型评估的时候，发现大不如意，得到的r2是负数
| 当时的代码 |

  ``` python
def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4, alpha=0.01):
    self.learning_rate = learning_rate 
    self.max_iter = max_iter
    self.tol = tol
    self.alpha = alpha  # L2 正则化参数
    self.history_learning_rate = []
       
def fit(self, X, y):
    self.theta = np.zeros(X.shape[1])
    self.cost_history = []   
    for epoch in range(1, self.max_iter + 1):
        y_pred = np.dot(X, self.theta)
        loss = np.mean((y_pred - y) ** 2) / 2 + self.alpha * np.sum(self.theta**2) / 2
        self.cost_history.append(loss)
        gradient = np.dot(X.T, (y_pred - y)) / len(y) + self.alpha * self.theta
        self.learning_rate = self.learning_rate / epoch
        self.history_learning_rate.append(self.learning_rate)
        self.theta -= self.learning_rate * gradient
        if len(self.cost_history) > 1 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tol:
            break
  ```

也不知道为什么，加了正则表达式之后，弄出了负相关，于是将正则表达式给删了（所以还是没弄懂为什么我要去掉才可以），才有我现在的梯度下降法，但是这费了我好长时间

------

# 2024年3月27日（周三）

## 今日学习总结

- 今天对多元线性回归的的数据分析进行总结
- 对于逻辑回归的实现

## 今日心得体会

直到下午，我把图反映出来的数据用文字描述了，但是我感觉描述的很抽象而且还不一定正确，所以就那样把，还有我觉得可视化的话，除了散点图我就没有什么想法将其可视化了，因为我觉得散点图可以描述两个变量之间的关系而且还可以描述变量的个数，一共有三个变量，还有热力图，可以描述各个变量之间的相关性，但是其他的图我想不到怎么描述(比如条形图只能描述一个变量的数量)，对，所以我觉得我的可视化还是做的一般般，没有说很清楚。把小组的基础作业弄好之后我就想着做做附加作业了，而且还是担心第三周的大组作业的，因为队列之后就是树了，树的实现我感觉对我来说还是很麻烦的，因为上次的队列和栈都弄得我汗流浃背了；晚上想着完成以下逻辑回归（我在西瓜书那了解到，逻辑回归还可以叫对数几率回归）的作业吧，然后由于我上个星期也提前看过逻辑回归的概念了，所以用起来就很快了，在网上参考了逻辑回归的模型，用了梯度下降法（还参考了别人的L2正则化），只能说梯度下降法如果在多元线性回归实现了的话，那么用在逻辑回归那还是很快的，而且在网上还找了以下数据集，但是也没有深入的分析（比如对逻辑回归模型的评测，当然我知道的有这些测评方法，比如**准确率**，**精确率**，**召回率**，**F1 分数**，**ROC 曲线和AUC**，**混淆矩阵**，但是我这次知识调用了sklearn库的这些方法，并没有做到自己敲代码实现，但我感觉应该能做得出来毕竟已经有公式摆在那了，还有个ROC和AUC图我就不知道怎么画了（以后再研究吧……））,所以今晚就差不多了

------

# 2024年3月28日（周四）

## 今日学习总结

- 简单实现Kmeans算法
- 简单对数据结构的树实现(其它别的树没实现，只实现了二叉树)
- 涉猎一丁点深度学习…….

## 今日心得体会

早上实现了KMeans算法但是发现应用应用性很差

```python
class KMeans:
    def __init__(self, n_clusters, iter=500):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.centroids = []

    def fit(self, X):
        self.centroids = self.initialize_centroids(X)
        for _ in range(self.iter):
            labels = self.assign_points(X, self.centroids)
            new_centroids = self._updatecentroids(X, labels)
            if np.allclose(self.centroids, new_centroids):
                break
            self.centroids = new_centroids

    def initialize_centroids(self, X):
        indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)
        return X[indices]
    @staticmethod
    def assign_points(X, centroids):
        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

    def update_centroids(self, X, labels):
        new_centroids = np.zeros_like(self.centroids)
        for i in range(self.n_clusters):
            new_centroids[i] = np.mean(X[labels == i], axis=0)
        return new_centroids

    def predict(self, X):
        distances = np.sqrt(((X - self.centroids[:, np.newaxis]) ** 2).sum(axis=2))
        labels = np.argmin(distances, axis=0)
        return labels
```

当时再iris数据集上使用时，发现

```python
    def _initialize_centroids(self, X):
        indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)
        return X[indices]
```

有问题，说是引索出错，但是我想的就是随机初始化中心，然后想了一会儿没想到，电脑突然蓝屏了，心态爆炸…..,所以就算了，不做也罢………(hhhhhhhh)然后战线转移去数据结构的树去了…..，之前看如果要对树操作的话要进行递归操作的，我当时以为很复杂，但是实现二叉树的时候感觉还是挺简单的，但是我还没有对二叉树进行左旋或者右旋那种操作，稍微搞了一会儿，搞完之后我就去学习深度学习（但是没做什么笔记，所以我上传的笔记很是丑陋，勉强凑凑数而已）了，我感觉开头还是很简单的，就是有些比如神经网络这些我也知识稍微了解，因为之前机器学习的进度是学到决策树的，再往后才是神经网络，所以这次的学习也是给我后面的学习打下铺垫，然后学到7章8章左右我才知道模型居然还能编译的，但我看完10章还是觉得对模型编译这东西不怎么熟。其实我感觉更多的就是对算法的熟悉，能做到说做就做那种就很好了，在这基础上还要去对算法进行优化(虽然这是后面的事了)

------

# 2024年3月29日（周五）

## 今日学习总结

- 

## 今日心得体会





------

# 2024年3月30日（周六）

## 今日学习总结

- 

## 今日心得体会



------

# 2024年3月31日（周日）

## 今日学习总结

- 

## 今日心得体会

